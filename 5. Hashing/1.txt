/*     Analysis of hashing is done always on the basis of loading factor (λ=no of elements/size of hash table)


 -> Hashing is used to implement dictionary with key and value pairs
 -> Hashing is also used to implement sets(set of keys)
 -> hash table always has unique values
 -> With the help of hashing:
      - searching,inserting,deletion takes: O(1) time

 -> Without hashing:

   1. linear search->  O(n)      any order of array
      binary search->  O(logn)   requires sorted order to search
      with hashing ->O(1)
              fastest method of searchng is hashing
                   hashing>binary search>linear
   2. insert -> O(n)
      with hashing -> O(1)
   3. delete -> O(n)
      with hashing -> O(1)


 Hashing is not used in:
   1. finding closest value     ->avl tree is used
   2. sorted data               ->avl tree is used
   3. prefix searching(strings) ->for this use trie data structure

 What hash function do:
 -should always map large values to keys that can be used as an index for direct address table
 -should generate values from 0 to m-1
 -should be O(1) for int and O(len) for strings
 -should uniformaly distribute the values in the hash table
*/
                                    EXAMPLES OF HASH FUNCTIONS:
/* 1.) Basic hash function     (one to one function: ideal hashing)
       h(large_key) =large_key % m
     (by creating bollen array)(Direct Address Table)(hash table)

 -> Hash table formed is called bollean array or Direct Address Table
 -> Size of hash table should be proportional to the size of input
    (eg. if input size is 100  then size of hash function should be proportional to 100 eg.100 or 200 or 300 and not 10000000 or 2 or 10 etc)
 ->generally take prime numbers as modulo(m) close or greater to the size of hash table in hash function
   (because we will will have less factor of the that number as it is prime so, helps in better distribution of keys)
   (
   -bad value of m is 10^x (or 100 or 1000 etc) as here we are only considering only last x digits of the key that  is a bad idea
   -also, don't consider m as 2^x  as we are only considering last x bits of binary representation of x
   -also, don't consider prime numbers close to powers    )

 Drawback of using basic hash function along with Direct Address Table:
 1. can't handle big keys(if phone number are keys)
 2. floating point , string as an index can't be created etc
 3. lot of space is required and lot of them is wasted: requires bigger array etc;


 SOLUTION FOR GREATER MEMORY CONSUMPTION:
  .take array of size 10 (0 to 9)
  .modify one-to-one hashing function to many-to-one hashing function
   eg. h(x)=x%10;

 DRAWBACK:
  if we modify ideal hash function then their is risk of collision i.e becomes many to one
  5,15,25,35......will be stored at index 5 -> collision

 SOLUTION TO COLLISION:

 a.) IF WE KNOW KEYS IN ADVANCE THEN THEIR WILL BE NO COLLISION IF WE USE "PERFECT HASHING"(advance topic)

 b.) TWO METHODS ARE THEIR TO HANDLE COLLISION IF KEY IS NOT KNOWN:

 1. open hashing: can consume extra memory than 10 size array
   ->chaining:
             creating pointer array that stores address of linked list.
             the linked list is inserted at that index in sorted order
            - Expected chain length =  λ
   . Loading factor: λ : (no. of elements)/(size of hash table)   : no of elemets at an index
          gives ideal distribution of elements in the hash table.
   . Expected Chain length = λ ( if keys are expected to be distributed uniformily )

   . Time for successfull search / delete:
      t = 1(for computing hash function h(x)=x%10 and getting the index) + λ/2(avg time to search in linked list)
          =O(1+ λ/2)
     Time for unsuccessfull search:
      t = 1(for computing hash function h(x)=x%10 and getting the index) + λ(max time to search in linked list)
         =O(1+ λ)
        .. Expected best time for insert/delete: O(1)  (if  λ=1 i.e uniform distribution)


   -> data structures to store the chain:
       i) linked list
             worst case search,delete,insert = O(n)
             not cache friendly
             extra space to store pointers
      ii) dynamic sized array  (eg. vectors)
             worst case search,delete,insert = O(n)
             cache friendly
      iii) self balanced bst (avl,red black trees)
             worst case search,delete,insert = O(logn)
             not cache friendly

 2. closed hashing (open addressing))
    ->no. of slots in hash table >= no. of keys to be inserted
    ->cache friendly

     . linear probing
      (get the index using hash function then linearly search for next empty slot when their is a collision in circular manner until we find the key (key present) or we get an empty slot(key absent) or we reach the same index as starting (key absent) )
      (deletion is complex as after that vaccant space is left and searching in disrupted:
       deletion disruption option is:
        i)  rehashing: delete the key and rehash the complete array
        ii) mark the deleted slot as deleted and continue searching)

        h(key)=[h1(key) + f(i)]%m            where h1(key)=key%m  and f(i)=i   ;i=0,1,2,3,......

        store the value at index h(key) and if collision takes palace then store the value at next empty space that can be found using h(key). Key not found if the index is empty

        avg. successfull search : t= (1/λ)log( 1/(1-λ) )
        avg. unsuccessfull search : t=  1/(1-λ)

       Drawback of linear probing:
       . keep half the hash table vaccant(wastage of space)
       . takes more time then chaining(primary clusters are formed )(solution is quadratic probing)
       . deletion is complex as after that vaccant space is left and searching in disrupted:
         deletion disruption option is rehashing: delete the key and rehash the complete array
         or  mark the deleted slot as deleted and continue searching)



    . Quadratic probing(better than linear probing)
      designed to avoide the primary clusters of elements as they did in linear probing
         h(key,i)=[h(key) + f(i)]%m            where h(key)=key%m   and f(i)=i^2   ;i=0,1,2,3,......

          avg. successfull search : t= -loge(1-λ) / λ
          avg. unsuccessfull search : t=  1/(1-λ)

          Quadratic probing gurantees searching of empty if:
          λ<0.5 and m is prime   => hash table size is atleast double the the size of input keys

          Drawback:
          .forms secondary clusters
          .may or may not find empty slots as no linear searching is done
           (quadratic probing gurantees to find key or empty slot if λ < 0.5 and % m is prime)

    . Double hashing (double collision)
      designed to avoide the clusters of elements as they did in linear probing and quadratic probing

         h(key,i)=[h1(key) + j*h2(key)]%m            where h1(key)=key%m   and h2(key)=(m-1)-( key%(m-1))
         number of times collision ouccers at an index  j=0,1,2,3......

   ->if h2(key) is relatively prime to 'm' then this hash function definetely finds empty space if their is one
   ->no clustering is formed
   ->distribute keys more uniformly then linear and quadratic probing
   ->λ <=1
   -> Time for successful search = 1/(1-λ)

   void double_hashing(key)
   {
      if (table_is_full)
       return error;

       probe=h1(key);
       offset=h2(key);
       while(table[probe] is occupied)
         probe=(probe+offset)%m;

      table[probe]=key;
   }
*/

/* 2.) for strings
     string s="abcd";
     h(s)=  (s[0]*(x^0) + s[1]*(x^1) + s[2]*(x^2) + s[3]*(x^3) ) % m              weighted sum
*/

/* 3.) UNIVERSAL HASH FUNCTIONS  (consists of group of hash FUNCTIONS)
     ->eg. unordered_set , unordered_map in c++

   -> it is most famous hashing technique
   -> All elements are stores in any order( elements are stored in random order)(depends on compiler)

   #include<unordered_set>

    unordered_set<int> s;
                                    (9 main functions:)
      O(1) on average:     (assumption that distribution is uniform)
    . s.insert(x)  -> inserts x in any order . Does not insert dupicate elements
    . s.find(x)    -> returns iterator to the element if present else return s.end()
    . s.count(x)   -> same as find but it returns 1 if the element is present and 0 if element is not present
    . s.erase(x)   -> deletes element x from the set . Size decreases by 1
                  -> x can be the actual element or iterator to an element
                  -> used to remove range of elements -> s.erase(s.begin()+1 , s.end()-2);
    . s.clear()    -> clears unordered_set

       O(1)
    . s.empty()
    . s.size()     -> no. of elements stored
    . s.begin()    -> iterator to the first element
    . s.end()      -> iterator to the next to last element


   #include<unordered_map>
    -> used to store key and value pairs
    -> uses hashing

    unordered_map<key,value> obj;
    //default value of value is 0;

    . [key]        -> member access operator;
                   -> returns reference of the value of the key if key is present
                   -> if key is absemt them it inserts the key and returns reference of the value

      obj["hi"]=10;                  // inserted key("hi") value(10) pair
    . obj.at(key)  ->performs similarly as [] but throws out of scope exception if key is not present
    . obj.insert({"harsh", 20 });   // inserted key value pair
      //obj.insert({"harsh"}); error
      obj["harsh"];  ->inserts at index "harsh" and value inserteed is 0
      obj["harsh"]++; ->value inserted at index "harsh" is 1; ->increased everytime
     find ,count,size ,erase function is also present
    . obj.first   -> prints key
    . obj.second  -> prints corresponding value
    if it is pointer
     it->first   and it->second
*/
